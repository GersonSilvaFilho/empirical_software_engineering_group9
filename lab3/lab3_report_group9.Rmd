---
title: "Empirical software engineering"
subtitle: |
  | Lab 3: Design and simulate an experiment
  | Group 9
output:
  pdf_document
author:
  - Anton Lutteman
  - Daniel Olsson
  - Gerson Silva Filho
  - Johan Mejborn
date: "`r format(Sys.time(), '%d %B %Y')`"
---
# Simulate the data

## 1) Simulate for a the sample size per group that you defined in the power analysis.

### a) How does your fitted model looks like?

* **Test Assumption 1 - Homoscedasticity**

We decided to run a Levene Test to test Homoscedasticity.

```{r 1_q_1}
car::leveneTest(m1)
```

The result show that we cannot reject homoscedasticity. Then let's also verify the QQ-Plot in order to get another point of view.

```{r 1_q_2}
car::qqPlot(m1)
```

QQplot shows that the data is homoscedastic.

* **Test Assumtion 2 - Normality**

In order to verify normality we run a Shapiro Wilk test.

```{r 1_q_3}
shapiro.test(m1$residuals)
```

The result show that we cannot reject normallity. And in order to verify it graphically we used
the QQ-plot of the residuals.

```{r 1_q_4}
car::qqPlot(m1$residuals)
```

The QQplot on the residuals shows normallity.

* **Test Assumtion 3 - Independence**

Based on how we collected the data, we can assume independence.

* **The analysis**

After all the assumptions have been met, we can run an ANOVA test.

```{r 1_q_5}
car::Anova(m1)
```

The ANOVA test shows that we can reject that Language, Experience, Language:IDE,
Language:Experience and IDE:Experience has the same mean for the levels because 
of the low p values.

**Hypothesis 1**: IDE alone doesn't seem to have an effect on LOC but we can see that it has a
significant effect in combination with Language and Experience. 

**Hypothesis 2**: Experience seem to have a significant effect on LOC based on the result.

**Hypothesis 3**: Language seem to have a significant effect on LOC bases on the result.

Then to get the highest combination for lines of code we can use the Tukey test:

```{r 1_q_6}
TukeyHSD(aov(m1))
```

Without drawing conclusions for all combinations presented by the Tukey test we can say that
the highest combination for lines of code is **Language = C++, IDE = Visual Studio and Experience = Senior**.

### b) How do it compare with your ‘true’ model you defined in the simulation.

When we compare the created model defined in our main function with the defined coefficients, with the data generated by the model, we can see that they are similar. Also we can notice some variations that can be explained either by the defined standard deviation of the model, or that the model interpreted the data in another way than what we set up. The difference in the first order is very small compared to our true model, however the third order interactions are deviating quite a bit compared to our true model. 

We can observe the differences looking in the following table and the defined factors:

```{r 1_b_1, echo=FALSE}
tableData = data.frame(facDefinition, coef(summary(m1))[, "Estimate"]) 
kable(tableData, col.names = c("Defined Coeficients", "Model Coeficients"))
```



## 2) Simulate for an underpowered study with half the sample size you calculated in the power analysis

### a) How does your fitted model looks like?

* **Test Assumption 1 - Homoscedasticity**

We decided to run a Levene Test to test Homoscedasticity.

```{r 2_q_1}
car::leveneTest(m2)
```

The result show that we cannot reject homoscedasticity. Then let's also verify the QQ-Plot in order to get another point of view.

```{r 2_q_2}
car::qqPlot(m2)
```

QQplot shows that the data is homoscedastic.

* **Test Assumtion 2 - Normality**

In order to verify normality we run a Shapiro Wilk test.

```{r 2_q_3}
shapiro.test(m2$residuals)
```

The result show that we cannot reject normallity. And in order to verify it graphically we used
the QQ-plot of the residuals.

```{r 2_q_4}
car::qqPlot(m2$residuals)
```

The QQplot on the residuals shows normallity.

* **Test Assumtion 3 - Independence**

Based on how we collected the data, we can assume independence.

* **The analysis**

After all the assumptions have been met, we can run an ANOVA test.

```{r 2_q_5}
car::Anova(m2)
```

The ANOVA test shows that we can reject that Language, Experience, Language:IDE,
Language:Experience and IDE:Experience has the same mean for the levels because 
of the low p values.

**Hypothesis 1**: IDE doesn't seem to have an effect on LOC neither alone nor as an interaction.

**Hypothesis 2**: Experience seem to have a significant effect on LOC based on the result.

**Hypothesis 3**: Language seem to have a significant effect on LOC bases on the result.

Then to get the highest combination for lines of code we can use the Tukey test:

```{r 2_q_6}
TukeyHSD(aov(m2))
```

Without drawing conclusions for all combinations presented by the Tukey test we can say that
the highest combination for lines of code is **Language = C++ and Experience = Senior**.

### b) How do it compare with your ‘true’ model you defined in the simulation

When we compare the created model defined in our main function with the defined coefficients, with the data generated by the model, we can see that they are similar. Also we can notice some variations that can be explained either by the defined standard deviation of the model, or that the model interpreted the data in another way than what we set up. The difference in the first order is very small compared to our true model, however the third order interactions are deviating quite a bit compared to our true model. 

We can observe the differences looking in the following table and the defined factors:

```{r 2_b_1, echo=FALSE}
tableData = data.frame(facDefinition, coef(summary(m2))[, "Estimate"]) 
kable(tableData, col.names = c("Defined Coeficients", "Model Coeficients"))
```

### c) How do the results change compared to the correctly powered experiment.

When we compare the two models we see some differences between them. These could 
be explained by the standard deviation and the small sample size. We also get 
more coefficients with small p values that are to be considered statistically 
significant for m1.

Comparing the analysis on the two models show similar result for Hypothesis 2 and 3 but 
for model two we do not see an interaction effect for IDE. This means we can't 
reject the null hypothesis for hypothesis 1 that the means are equal for the different IDEs.
